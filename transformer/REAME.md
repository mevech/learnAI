# Simple repo to collect my notes on AI as an interest

## GOD TIER

[Attention is all you need](https://arxiv.org/pdf/1706.03762)

## Learning Materials

Let's build GPT: from scratch, in code, spelled out. By Andrej Karpathy [video](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2456s&ab_channel=AndrejKarpathy)
Huggingface transformer course. [course](https://huggingface.co/learn/llm-course/chapter1/1)

## Questions to understand

- [ ] Steps for pre-training
- [ ] Steps for inferencing
- [ ] What are the overlapping algorithm between pre-training and fine-tuning?
- [ ] How does the model decide to stop generating?
- [ ] What is multihead latent self attention?
- [ ] How is mixture of experts architecture work?
